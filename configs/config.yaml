mode: train
log_dir: ~/fomo_logdir

pretrained_path: ~/fomo_logdir/model.pth
model_name: clip
backbone_name: vit

backbone_config:
  mode: original
  num_classes: 2
  inc: 3
  dropout: false

data_augmentation:
  enable: true
  augmentations:
    flip_prob: 0.5
    rotate_prob: 0.5
    rotate_limit: [-10, 10]
    blur_prob: 0.5
    blur_limit: [3, 7]
    brightness_prob: 0.5
    brightness_limit: [-0.1, 0.1]
    contrast_limit: [-0.1, 0.1]
    quality_lower: 40
    quality_upper: 100

clipping:
  coop:
    n_ctx: 16
    csc: false
    ctx_init: ""
    prec: "fp16"
    class_token_position: "end"
  dataloader:
    train:
      batch_size: 16
    test:
      batch_size: 100
    num_workers: 8

  input:
    size: [224, 224]
    interpolation: "bicubic"
    pixel_mean: [0.48145466, 0.4578275, 0.40821073]
    pixel_std: [0.26862954, 0.26130258, 0.27577711]
    transforms:
      ["random_resized_crop", "random_flip", "normalize", "compress_blur"]

  optim:
    name: "sgd"
    lr: 0.002
    max_epoch: 2
    lr_scheduler: "cosine"
    warmup_epoch: 1
    warmup_type: "constant"
    warmup_cons_lr: 1.0e-5
    warmup_min_lr: 1.0e-5
    stepsize: [-1]
    gamma: 0.1
    momentum: 0.9
    weight_decay: 5.0e-4
    sgd_dampning: 0
    sgd_nesterov: false
    rmsprop_alpha: 0.99
    adam_beta1: 0.9
    adam_beta2: 0.999
    staged_lr: false
    new_layers: []
    base_lr_mult: 0.1

  train:
    print_freq: 5

  model:
    backbone:
      name: "vit-l/14"

dataset:
  name: DF40
  root: /scratch-shared/scur0555/datasets/clipping_the_deception/
  manifest: /scratch-shared/scur0555/datasets/clipping_the_deception/manifest.json
  train_batch_size: 32
  test_batch_size: 32
  num_workers: 8
  jpeg_quality: 95 # .png to .jpeg quality for training and testing
  frame_num: { train: 8, test: 8 } # number of frames to use per video in training and testing
  resolution: 224 # resolution of output image to network
  clip_size: 16 # number of frames in each clip
  class_to_label: { fake: 1, real: 0 }
  mean: [0.5, 0.5, 0.5]
  std: [0.5, 0.5, 0.5]

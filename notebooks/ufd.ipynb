{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95cce66f",
   "metadata": {},
   "source": [
    "### Universal Fake Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d3eac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffeff15",
   "metadata": {},
   "source": [
    "### Testing OpenCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7c92aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/scur0555/.conda/envs/fomo/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/scur0555/.conda/envs/fomo/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/scur0555/.conda/envs/fomo/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3362, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3607, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/scratch-local/scur0555.11471682/ipykernel_4064415/346515099.py\", line 3, in <module>\n",
      "    import open_clip\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/open_clip/__init__.py\", line 3, in <module>\n",
      "    from .coca_model import CoCa\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/open_clip/coca_model.py\", line 9, in <module>\n",
      "    from .transformer import (\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/open_clip/transformer.py\", line 10, in <module>\n",
      "    from .utils import to_2tuple, feature_take_indices\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/open_clip/utils.py\", line 8, in <module>\n",
      "    from torchvision.ops.misc import FrozenBatchNorm2d\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/torchvision/__init__.py\", line 6, in <module>\n",
      "    from torchvision import datasets, io, models, ops, transforms, utils\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/torchvision/models/__init__.py\", line 17, in <module>\n",
      "    from . import detection, optical_flow, quantization, segmentation, video\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/torchvision/models/detection/__init__.py\", line 1, in <module>\n",
      "    from .faster_rcnn import *\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/torchvision/models/detection/faster_rcnn.py\", line 16, in <module>\n",
      "    from .anchor_utils import AnchorGenerator\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/torchvision/models/detection/anchor_utils.py\", line 10, in <module>\n",
      "    class AnchorGenerator(nn.Module):\n",
      "  File \"/home/scur0555/.local/lib/python3.11/site-packages/torchvision/models/detection/anchor_utils.py\", line 63, in AnchorGenerator\n",
      "    device: torch.device = torch.device(\"cpu\"),\n",
      "/home/scur0555/.local/lib/python3.11/site-packages/torchvision/models/detection/anchor_utils.py:63: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(\"cpu\"),\n",
      "/home/scur0555/.conda/envs/fomo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from models.clip_models import CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22f8d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)ip_pytorch_model.bin: 100%|██████████| 1.71G/1.71G [00:06<00:00, 263MB/s]\n",
      "/home/scur0555/.local/lib/python3.11/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n",
      "/home/scur0555/.local/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "name = \"ViT-L/14\"\n",
    "pretrained = \"dfn2b\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(name, pretrained=pretrained)\n",
    "model.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc7dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Dummy dataset of random floats\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, size=2):\n",
    "        # random images\n",
    "        self.data = torch.rand(size, 3, 224, 224)\n",
    "        # binary labels\n",
    "        self.labels = torch.randint(0, 2, (size, 1)).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "\n",
    "# Provided validate function\n",
    "def validate(model, loader, find_thres=False):\n",
    "    with torch.no_grad():\n",
    "        y_true, y_pred = [], []\n",
    "        print(\"Length of dataset: %d\" % (len(loader)))\n",
    "        for img, label in loader:\n",
    "            in_tens = img.cpu()\n",
    "            y_pred.extend(model(in_tens).sigmoid().flatten().tolist())\n",
    "            y_true.extend(label.flatten().tolist())\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    print(\"True labels:\", y_true)\n",
    "    print(\"Predicted scores:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d969ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = {\n",
    "    \"RN50\" : 1024,\n",
    "    \"ViT-L/14\" : 768,\n",
    "    \"ViT-H/14\" : 1024,\n",
    "    \"ViT-g/14\" : 1024,\n",
    "}\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, name, pretrained=None, num_classes=1):\n",
    "        super(CLIPModel, self).__init__()\n",
    "        self.name = name\n",
    "        # self.preprecess will not be used during training, which is handled in Dataset class\n",
    "        if pretrained:\n",
    "            self.model, _, self.preprocess = open_clip.create_model_and_transforms(name, \n",
    "                                                                            pretrained=pretrained,\n",
    "                                                                            device=\"cpu\")\n",
    "        else:\n",
    "            self.model, self.preprocess = clip.load(name, device=\"cpu\")\n",
    "\n",
    "        # add a linear layer to the model (hard-coded for ViT)\n",
    "        self.project = nn.Linear(1024, 768)\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    " \n",
    "\n",
    "    def forward(self, x, return_feature=False):\n",
    "        features = self.model.encode_image(x)\n",
    "        if CHANNELS.get(self.name) == 1024: \n",
    "            features = self.project(features)\n",
    "        if return_feature:\n",
    "            return features\n",
    "        return self.fc(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55712114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 2\n",
      "True labels: [0. 1.]\n",
      "Predicted scores: [0.36211863 0.43808025]\n"
     ]
    }
   ],
   "source": [
    "name = \"ViT-H/14\"\n",
    "pretrained = \"laion2b_s32b_b79k\"\n",
    "\n",
    "# Create data loader and model\n",
    "dataset = DummyDataset()\n",
    "loader = DataLoader(dataset, batch_size=1)\n",
    "# Load model\n",
    "model = CLIPModel(name, pretrained)\n",
    "state_dict = torch.load(\"../pretrained_weights/fc_weights.pth\", map_location='cpu')\n",
    "model.fc.load_state_dict(state_dict)\n",
    "model = model.cpu()\n",
    "\n",
    "# Run test\n",
    "validate(model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0381042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCLIP test (optional)\n",
    "image = preprocess(Image.open(\"docs/CLIP.png\")).unsqueeze(0)\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n",
    "\n",
    "with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc3a122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m'),\n",
       " ('RN50x4', 'openai'),\n",
       " ('RN50x16', 'openai'),\n",
       " ('RN50x64', 'openai'),\n",
       " ('ViT-B-32', 'openai'),\n",
       " ('ViT-B-32', 'laion400m_e31'),\n",
       " ('ViT-B-32', 'laion400m_e32'),\n",
       " ('ViT-B-32', 'laion2b_e16'),\n",
       " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
       " ('ViT-B-32', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-B-32', 'datacomp_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_clip_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_laion_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_image_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_text_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_basic_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'datacomp_s_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_clip_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_laion_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_image_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_text_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_basic_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_s13m_b4k'),\n",
       " ('ViT-B-32', 'metaclip_400m'),\n",
       " ('ViT-B-32', 'metaclip_fullcc'),\n",
       " ('ViT-B-32-256', 'datacomp_s34b_b86k'),\n",
       " ('ViT-B-16', 'openai'),\n",
       " ('ViT-B-16', 'laion400m_e31'),\n",
       " ('ViT-B-16', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'laion2b_s34b_b88k'),\n",
       " ('ViT-B-16', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-B-16', 'datacomp_l_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_clip_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_laion_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_image_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_text_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_basic_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_s1b_b8k'),\n",
       " ('ViT-B-16', 'dfn2b'),\n",
       " ('ViT-B-16', 'metaclip_400m'),\n",
       " ('ViT-B-16', 'metaclip_fullcc'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'openai'),\n",
       " ('ViT-L-14', 'laion400m_e31'),\n",
       " ('ViT-L-14', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
       " ('ViT-L-14', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_clip_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_laion_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_s13b_b90k'),\n",
       " ('ViT-L-14', 'metaclip_400m'),\n",
       " ('ViT-L-14', 'metaclip_fullcc'),\n",
       " ('ViT-L-14', 'dfn2b'),\n",
       " ('ViT-L-14', 'dfn2b_s39b'),\n",
       " ('ViT-L-14-336', 'openai'),\n",
       " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
       " ('ViT-H-14', 'metaclip_fullcc'),\n",
       " ('ViT-H-14', 'metaclip_altogether'),\n",
       " ('ViT-H-14', 'dfn5b'),\n",
       " ('ViT-H-14-378', 'dfn5b'),\n",
       " ('ViT-g-14', 'laion2b_s12b_b42k'),\n",
       " ('ViT-g-14', 'laion2b_s34b_b88k'),\n",
       " ('ViT-bigG-14', 'laion2b_s39b_b160k'),\n",
       " ('ViT-bigG-14', 'metaclip_fullcc'),\n",
       " ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n",
       " ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n",
       " ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k'),\n",
       " ('convnext_base', 'laion400m_s13b_b51k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k_augreg'),\n",
       " ('convnext_base_w', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k_augreg'),\n",
       " ('convnext_large_d', 'laion2b_s26b_b102k_augreg'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft_soup'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_rewind'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_soup'),\n",
       " ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('EVA01-g-14', 'laion400m_s11b_b41k'),\n",
       " ('EVA01-g-14-plus', 'merged2b_s11b_b114k'),\n",
       " ('EVA02-B-16', 'merged2b_s8b_b131k'),\n",
       " ('EVA02-L-14', 'merged2b_s4b_b131k'),\n",
       " ('EVA02-L-14-336', 'merged2b_s6b_b61k'),\n",
       " ('EVA02-E-14', 'laion2b_s4b_b115k'),\n",
       " ('EVA02-E-14-plus', 'laion2b_s9b_b144k'),\n",
       " ('ViT-B-16-SigLIP', 'webli'),\n",
       " ('ViT-B-16-SigLIP-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP-i18n-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP-384', 'webli'),\n",
       " ('ViT-B-16-SigLIP-512', 'webli'),\n",
       " ('ViT-L-16-SigLIP-256', 'webli'),\n",
       " ('ViT-L-16-SigLIP-384', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP', 'webli'),\n",
       " ('ViT-SO400M-16-SigLIP-i18n-256', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP-378', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP-384', 'webli'),\n",
       " ('ViT-B-32-SigLIP2-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP2', 'webli'),\n",
       " ('ViT-B-16-SigLIP2-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP2-384', 'webli'),\n",
       " ('ViT-B-16-SigLIP2-512', 'webli'),\n",
       " ('ViT-L-16-SigLIP2-256', 'webli'),\n",
       " ('ViT-L-16-SigLIP2-384', 'webli'),\n",
       " ('ViT-L-16-SigLIP2-512', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP2', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP2-378', 'webli'),\n",
       " ('ViT-SO400M-16-SigLIP2-256', 'webli'),\n",
       " ('ViT-SO400M-16-SigLIP2-384', 'webli'),\n",
       " ('ViT-SO400M-16-SigLIP2-512', 'webli'),\n",
       " ('ViT-gopt-16-SigLIP2-256', 'webli'),\n",
       " ('ViT-gopt-16-SigLIP2-384', 'webli'),\n",
       " ('ViT-L-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-L-14-CLIPA-336', 'datacomp1b'),\n",
       " ('ViT-H-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-H-14-CLIPA-336', 'laion2b'),\n",
       " ('ViT-H-14-CLIPA-336', 'datacomp1b'),\n",
       " ('ViT-bigG-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-bigG-14-CLIPA-336', 'datacomp1b'),\n",
       " ('nllb-clip-base', 'v1'),\n",
       " ('nllb-clip-large', 'v1'),\n",
       " ('nllb-clip-base-siglip', 'v1'),\n",
       " ('nllb-clip-base-siglip', 'mrl'),\n",
       " ('nllb-clip-large-siglip', 'v1'),\n",
       " ('nllb-clip-large-siglip', 'mrl'),\n",
       " ('MobileCLIP-S1', 'datacompdr'),\n",
       " ('MobileCLIP-S2', 'datacompdr'),\n",
       " ('MobileCLIP-B', 'datacompdr'),\n",
       " ('MobileCLIP-B', 'datacompdr_lt'),\n",
       " ('ViTamin-S', 'datacomp1b'),\n",
       " ('ViTamin-S-LTT', 'datacomp1b'),\n",
       " ('ViTamin-B', 'datacomp1b'),\n",
       " ('ViTamin-B-LTT', 'datacomp1b'),\n",
       " ('ViTamin-L', 'datacomp1b'),\n",
       " ('ViTamin-L-256', 'datacomp1b'),\n",
       " ('ViTamin-L-336', 'datacomp1b'),\n",
       " ('ViTamin-L-384', 'datacomp1b'),\n",
       " ('ViTamin-L2', 'datacomp1b'),\n",
       " ('ViTamin-L2-256', 'datacomp1b'),\n",
       " ('ViTamin-L2-336', 'datacomp1b'),\n",
       " ('ViTamin-L2-384', 'datacomp1b'),\n",
       " ('ViTamin-XL-256', 'datacomp1b'),\n",
       " ('ViTamin-XL-336', 'datacomp1b'),\n",
       " ('ViTamin-XL-384', 'datacomp1b'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m'),\n",
       " ('RN50-quickgelu', 'cc12m'),\n",
       " ('RN101-quickgelu', 'openai'),\n",
       " ('RN101-quickgelu', 'yfcc15m'),\n",
       " ('RN50x4-quickgelu', 'openai'),\n",
       " ('RN50x16-quickgelu', 'openai'),\n",
       " ('RN50x64-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
       " ('ViT-B-32-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-B-32-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-B-16-quickgelu', 'openai'),\n",
       " ('ViT-B-16-quickgelu', 'dfn2b'),\n",
       " ('ViT-B-16-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-B-16-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-L-14-quickgelu', 'openai'),\n",
       " ('ViT-L-14-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-L-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-L-14-quickgelu', 'dfn2b'),\n",
       " ('ViT-L-14-336-quickgelu', 'openai'),\n",
       " ('ViT-H-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-H-14-quickgelu', 'dfn5b'),\n",
       " ('ViT-H-14-378-quickgelu', 'dfn5b'),\n",
       " ('ViT-bigG-14-quickgelu', 'metaclip_fullcc')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import open_clip\n",
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdd3c95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {\"a\": 1}\n",
    "dic.get(\"a\", 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0cb1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fomo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
